{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine translator v/01\n",
    "First attempt to make a seq2seq machine translator. \n",
    "Credits, inspiration, and thanks to:\n",
    "- Hvass @ https://github.com/Hvass-Labs\n",
    "- Challet @ https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "- Keras Blog https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "Ideas for improvment:\n",
    "- Reverse word sequence in source text\n",
    "- Bi-directional RNN\n",
    "- Attention mechanism\n",
    "- Save Tokenizer: https://stackoverflow.com/questions/45735070/keras-text-preprocessing-saving-tokenizer-object-to-file-for-scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThomasGordon\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2-tf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Model\n",
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "Load data from the internet and extract from tar-file. Data from http://www.statmt.org/europarl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "# data location in my system and on the internet\n",
    "data_dir = \"data/europarl/\"\n",
    "data_url = \"http://www.statmt.org/europarl/v7/\"\n",
    "\n",
    "# full url to data\n",
    "url = data_url + 'da' + \"-en.tgz\"\n",
    "\n",
    "# function to print download progress\n",
    "def _print_download_progress(count, block_size, total_size):\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "    pct_complete = min(1.0, pct_complete)\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# set file name and \"save\" path\n",
    "filename = url.split('/')[-1]\n",
    "file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "# if file does not exist, then download and extract\n",
    "if not os.path.exists(file_path):\n",
    "    \n",
    "    # make dir, if not exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    # Download the file from the internet.\n",
    "    file_path, _ = urllib.request.urlretrieve(url=url, filename=file_path, reporthook=_print_download_progress)\n",
    "    print()\n",
    "    print(\"Download finished. Extracting files.\")\n",
    "\n",
    "    # unzip or untar\n",
    "    if file_path.endswith(\".zip\"):\n",
    "        zipfile.ZipFile(file=file_path, mode=\"r\").extractall(data_dir)\n",
    "    elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "        tarfile.open(name=file_path, mode=\"r:gz\").extractall(data_dir)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"Data has apparently already been downloaded and unpacked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source and destination text into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markers to mark start and end of destination texts\n",
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source into a table\n",
    "filename = \"europarl-v7.da-en.da\"\n",
    "path = os.path.join(data_dir, filename)\n",
    "with open(path, encoding=\"utf-8\") as file:\n",
    "    # Read the line from file, strip leading and trailing whitespace,\n",
    "    # prepend the start-text and append the end-text.\n",
    "    data_src = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination into a table\n",
    "filename = \"europarl-v7.da-en.en\"\n",
    "path = os.path.join(data_dir, filename)\n",
    "with open(path, encoding=\"utf-8\") as file:\n",
    "    # Read the line from file, strip leading and trailing whitespace,\n",
    "    # prepend the start-text and append the end-text.\n",
    "    data_dest = [mark_start + line.strip() + mark_end for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Som De kan se, indfandt det store \"år 2000-problem\" sig ikke. Til gengæld har borgerne i en del af medlemslandene været ramt af meget forfærdelige naturkatastrofer.\n",
      "ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(data_src[i])\n",
    "print(data_dest[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dataset size\n",
    "to reduce training time dureing building model and experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size:    1968800 1968800\n",
      "New lighter dataset size: 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print\n",
    "print('Original dataset size:   ', len(data_src), len(data_dest))\n",
    "dataSetSize = 10000\n",
    "data_src = data_src[:dataSetSize]\n",
    "data_dest = data_dest[:dataSetSize]\n",
    "print('New lighter dataset size:', len(data_src), len(data_dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and pad SOURCE language\n",
    "Key outputs are:\n",
    "- \"data_src\", raw text inputs\n",
    "- \"tokens_padded_src\", a table that contains all the padded texts converted to tokens\n",
    "- \"tokens_to_string_src\", a function that transforms a token-list to a readable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16800 unique source tokens.\n"
     ]
    }
   ],
   "source": [
    "# crate source tokenizer and create vocabulary from the texts\n",
    "tokenizer_src = Tokenizer(num_words=num_words)\n",
    "tokenizer_src.fit_on_texts(data_src)\n",
    "print('Found %s unique source tokens.' % len(tokenizer_src.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate from words to tokens\n",
    "tokens_src = tokenizer_src.texts_to_sequences(data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten the longest tokens, Find the length of all sentences, truncate after 2 * std deviation\n",
    "num_tokens = [len(x) for x in tokens_src]\n",
    "max_tokens_src = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens_src = int(max_tokens_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 51)\n"
     ]
    }
   ],
   "source": [
    "# Pad / truncate all token-sequences to the given length.\n",
    "# This creates a 2-dim numpy matrix that is easier to use.\n",
    "tokens_padded_src = pad_sequences(tokens_src,\n",
    "                                  maxlen=max_tokens_src,\n",
    "                                  padding='post',\n",
    "                                  truncating='post')\n",
    "print(tokens_padded_src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverse lookup from integer-tokens to words\n",
    "index_to_word_src = dict(zip(tokenizer_src.word_index.values(), tokenizer_src.word_index.keys()))\n",
    "\n",
    "# function to return readable text from tokens string\n",
    "def tokens_to_string_src(tokens):\n",
    "    words = [index_to_word_src[token] \n",
    "            for token in tokens\n",
    "            if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'som de kan se indfandt det store år 2000 problem sig ikke til gengæld har borgerne i en del af medlemslandene været ramt af meget forfærdelige naturkatastrofer'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo to show that it works\n",
    "idx = 2\n",
    "tokens_to_string_src(tokens_padded_src[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Som De kan se, indfandt det store \"år 2000-problem\" sig ikke. Til gengæld har borgerne i en del af medlemslandene været ramt af meget forfærdelige naturkatastrofer.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14,    9,   24,  138, 8374,    4,  124,   77,  171,  399,   34,\n",
       "         19,    8, 2090,   20,  280,    3,   10,  165,    7,  778,  120,\n",
       "       1101,    7,   45, 4008, 1939,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_padded_src[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and pad DESTINATION language\n",
    "Key outputs are:\n",
    "- \"data_dest\", raw text inputs\n",
    "- \"tokens_padded_dest\", a table that contains all the padded texts converted to tokens\n",
    "- \"tokens_to_string_dest\", a function that transforms a token-list to a readable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10902 unique destination tokens.\n"
     ]
    }
   ],
   "source": [
    "# crate destination tokenizer and create vocabulary from the texts\n",
    "tokenizer_dest = Tokenizer(num_words=num_words)\n",
    "tokenizer_dest.fit_on_texts(data_dest)\n",
    "print('Found %s unique destination tokens.' % len(tokenizer_dest.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate from words to tokens\n",
    "tokens_dest = tokenizer_dest.texts_to_sequences(data_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten the longest tokens, Find the length of all sentences, truncate after 2 * std deviation\n",
    "num_tokens = [len(x) for x in tokens_dest]\n",
    "max_tokens_dest = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens_dest = int(max_tokens_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 58)\n"
     ]
    }
   ],
   "source": [
    "# Pad / truncate all token-sequences to the given length.\n",
    "# This creates a 2-dim numpy matrix that is easier to use.\n",
    "tokens_padded_dest = pad_sequences(tokens_dest,\n",
    "                                   maxlen=max_tokens_dest,\n",
    "                                   padding='post',\n",
    "                                   truncating='post')\n",
    "print(tokens_padded_dest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverse lookup from integer-tokens to words\n",
    "index_to_word_dest = dict(zip(tokenizer_dest.word_index.values(), tokenizer_dest.word_index.keys()))\n",
    "\n",
    "# function to return readable text from tokens string\n",
    "def tokens_to_string_dest(tokens):\n",
    "    words = [index_to_word_dest[token] \n",
    "            for token in tokens\n",
    "            if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end marks as tokens\n",
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_end = tokenizer_dest.word_index[mark_end.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ssss although as you will have seen the dreaded 'millennium bug' failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful eeee\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo to show that it works\n",
    "idx = 2\n",
    "tokens_to_string_dest(tokens_padded_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,  390,   21,   35,   24,   20,  592,    1, 6710, 6711, 6712,\n",
       "       1757,    5, 4291,  186,    1,   93,    7,    9,  246,    4,  120,\n",
       "       1967,    9, 1599,    4, 1032,  871,   10, 1216,  115, 4292,    3,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_padded_dest[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "- Input to the encoder is simply the source language as it is\n",
    "- Inputs to the decoder are slightly more complicated, since the two input strings are shiften one time-step: The model has to learn to predict the \"next\" token in the output from the input. Slizing is used to get two \"views\" to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 51)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = tokens_padded_src\n",
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 57)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data = tokens_padded_dest[:, :-1]\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 57)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data = tokens_padded_dest[:, 1:]\n",
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples showing the training data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,  390,   21,   35,   24,   20,  592,    1, 6710, 6711, 6712,\n",
       "       1757,    5, 4291,  186,    1,   93,    7,    9,  246,    4,  120,\n",
       "       1967,    9, 1599,    4, 1032,  871,   10, 1216,  115, 4292,    3,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "decoder_input_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 390,   21,   35,   24,   20,  592,    1, 6710, 6711, 6712, 1757,\n",
       "          5, 4291,  186,    1,   93,    7,    9,  246,    4,  120, 1967,\n",
       "          9, 1599,    4, 1032,  871,   10, 1216,  115, 4292,    3,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ssss although as you will have seen the dreaded 'millennium bug' failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful eeee\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string_dest(decoder_input_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"although as you will have seen the dreaded 'millennium bug' failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful eeee\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string_dest(decoder_output_data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neural Network\n",
    "### Create the Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input, Embedding, GRU, Dense\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ThomasGordon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py:1456: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, None)              0         \n",
      "_________________________________________________________________\n",
      "encoder_embedding (Embedding (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "encoder_gru1 (GRU)           (None, None, 512)         984576    \n",
      "_________________________________________________________________\n",
      "encoder_gru2 (GRU)           (None, None, 512)         1574400   \n",
      "_________________________________________________________________\n",
      "encoder_gru3 (GRU)           (None, 512)               1574400   \n",
      "=================================================================\n",
      "Total params: 5,413,376\n",
      "Trainable params: 5,413,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# network sizes\n",
    "embedding_size = 128\n",
    "state_size = 512\n",
    "\n",
    "# connect encoder\n",
    "encoder_input = Input(shape=(None, ), name='encoder_input')\n",
    "net = Embedding(input_dim=num_words, output_dim=embedding_size, name='encoder_embedding')(encoder_input)\n",
    "net = GRU(state_size, name='encoder_gru1', return_sequences=True)(net)\n",
    "net = GRU(state_size, name='encoder_gru2', return_sequences=True)(net)\n",
    "net = GRU(state_size, name='encoder_gru3', return_sequences=False)(net)\n",
    "encoder_output = net\n",
    "\n",
    "# Encoder model\n",
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_output])\n",
    "model_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Decoder model\n",
    "Create the decoder-part which maps the \"thought vector\" to a sequence of integer-tokens. The decoder takes two inputs. First it needs the \"thought vector\" produced by the encoder which summarizes the contents of the input-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 128)    1280000     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_initial_state (InputLay (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru1 (GRU)              (None, None, 512)    984576      decoder_embedding[0][0]          \n",
      "                                                                 decoder_initial_state[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru2 (GRU)              (None, None, 512)    1574400     decoder_gru1[0][0]               \n",
      "                                                                 decoder_initial_state[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru3 (GRU)              (None, None, 512)    1574400     decoder_gru2[0][0]               \n",
      "                                                                 decoder_initial_state[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 10000)  5130000     decoder_gru3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,543,376\n",
      "Trainable params: 10,543,376\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initial state for the decoder (given from encoder)\n",
    "decoder_initial_state = Input(shape=(state_size,), name='decoder_initial_state')\n",
    "\n",
    "# input to decoder (destination text given to estimate next word)\n",
    "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
    "\n",
    "# connect decoder\n",
    "net  = Embedding(input_dim=num_words, output_dim=embedding_size, name='decoder_embedding')(decoder_input)\n",
    "net = GRU(state_size, name='decoder_gru1', return_sequences=True)(net, initial_state=decoder_initial_state)\n",
    "net = GRU(state_size, name='decoder_gru2', return_sequences=True)(net, initial_state=decoder_initial_state)\n",
    "net = GRU(state_size, name='decoder_gru3', return_sequences=True)(net, initial_state=decoder_initial_state)\n",
    "decoder_output = Dense(num_words, activation='linear', name='decoder_output')(net)\n",
    "\n",
    "# Decoder model\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
    "                      outputs=[decoder_output])\n",
    "model_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TRAINING model\n",
    "This model connect from the input language to the translated language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 128)    1280000     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru1 (GRU)              (None, None, 512)    984576      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru2 (GRU)              (None, None, 512)    1574400     encoder_gru1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 128)    1280000     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru3 (GRU)              (None, 512)          1574400     encoder_gru2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru1 (GRU)              (None, None, 512)    984576      decoder_embedding[0][0]          \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru2 (GRU)              (None, None, 512)    1574400     decoder_gru1[0][0]               \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru3 (GRU)              (None, None, 512)    1574400     decoder_gru2[0][0]               \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 10000)  5130000     decoder_gru3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 15,956,752\n",
      "Trainable params: 15,956,752\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input to decoder (destination text given to estimate next word)\n",
    "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
    "\n",
    "# embedding layer\n",
    "net  = Embedding(input_dim=num_words, output_dim=embedding_size, name='decoder_embedding')(decoder_input)\n",
    "\n",
    "# connect training model\n",
    "net = GRU(state_size, name='decoder_gru1', return_sequences=True)(net, initial_state=encoder_output)\n",
    "net = GRU(state_size, name='decoder_gru2', return_sequences=True)(net, initial_state=encoder_output)\n",
    "net = GRU(state_size, name='decoder_gru3', return_sequences=True)(net, initial_state=encoder_output)\n",
    "decoder_output = Dense(num_words, activation='linear', name='decoder_output')(net)\n",
    "\n",
    "# Model to train the network\n",
    "model_train = Model(inputs=[encoder_input, decoder_input],\n",
    "                    outputs=[decoder_output])\n",
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    # Calculate the loss. This outputs a 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ThomasGordon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "optimizeroptimize  = RMSprop(lr=1e-3)\n",
    "decoder_targetdecoder_  = tf.placeholder(dtype='int32', shape=(None, None))\n",
    "model_train.compile(optimizer=optimizer,\n",
    "                    loss=sparse_cross_entropy,\n",
    "                    target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'tgc_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)\n",
    "\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 4.4859Epoch 00001: val_loss improved from inf to 3.41717, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 19s 2ms/step - loss: 4.4824 - val_loss: 3.4172\n",
      "\n",
      "Epoch 2/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 3.5811Epoch 00002: val_loss improved from 3.41717 to 3.37870, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 3.5806 - val_loss: 3.3787\n",
      "\n",
      "Epoch 3/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 3.4133Epoch 00003: val_loss improved from 3.37870 to 2.94673, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 3.4114 - val_loss: 2.9467\n",
      "\n",
      "Epoch 4/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 3.0434Epoch 00004: val_loss improved from 2.94673 to 2.86215, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 3.0438 - val_loss: 2.8622\n",
      "\n",
      "Epoch 5/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 2.9526Epoch 00005: val_loss improved from 2.86215 to 2.80908, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 2.9538 - val_loss: 2.8091\n",
      "\n",
      "Epoch 6/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 2.8980Epoch 00006: val_loss did not improve\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 2.8992 - val_loss: 2.8680\n",
      "\n",
      "Epoch 7/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 2.8790Epoch 00007: val_loss improved from 2.80908 to 2.75290, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 2.8780 - val_loss: 2.7529\n",
      "\n",
      "Epoch 8/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 2.8765Epoch 00008: val_loss improved from 2.75290 to 2.74991, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 2.8776 - val_loss: 2.7499\n",
      "\n",
      "Epoch 9/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 2.8265Epoch 00009: val_loss improved from 2.74991 to 2.68227, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 2.8262 - val_loss: 2.6823\n",
      "\n",
      "Epoch 10/10\n",
      "8960/9000 [============================>.]8960/9000 [============================>.] - ETA: 0s - loss: 2.7710Epoch 00010: val_loss improved from 2.68227 to 2.64091, saving model to tgc_checkpoint.keras\n",
      "9000/9000 [==============================]9000/9000 [==============================] - 17s 2ms/step - loss: 2.7710 - val_loss: 2.6409\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1e481873e10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = {'encoder_input': encoder_input_data, 'decoder_input': decoder_input_data}\n",
    "y_data = {'decoder_output' : decoder_output_data}\n",
    "\n",
    "model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                batch_size=640,\n",
    "                epochs=10,\n",
    "                validation_split=0.1,\n",
    "                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "\n",
    "    # tokenize the text to be translated\n",
    "    input_tokens = tokenizer_src.texts_to_sequences([input_text])\n",
    "    input_tokens = pad_sequences(input_tokens,\n",
    "                                 maxlen=max_tokens_dest,\n",
    "                                 padding='post',\n",
    "                                 truncating='post')\n",
    "    \n",
    "    # calculate thought vector\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "    \n",
    "    # create placeholder for translated text\n",
    "    shape = (1, max_tokens_dest)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "    \n",
    "    # set helper variables\n",
    "    token_int = token_start\n",
    "    output_text = ''\n",
    "    count_tokens = 0\n",
    "    \n",
    "    while token_int != token_end and count_tokens < max_tokens_dest:\n",
    "                   \n",
    "        # decoder input, initially hust the \"ssss\" marker as a token\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "            \n",
    "        # wrap data for clearity\n",
    "        x_data = {'decoder_initial_state': initial_state, 'decoder_input': decoder_input_data}\n",
    "            \n",
    "        # run decoder to predict next word\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "            \n",
    "        # get the last predicted token\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        \n",
    "        # convert to an integer token, the index in the one-hot\n",
    "        token_int = np.argmax(token_onehot)\n",
    "            \n",
    "        # lookup the word in plain letters\n",
    "        sampled_word = index_to_word_src[token_int]\n",
    "            \n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "            \n",
    "        # Increment the token-counter\n",
    "        count_tokens += 1\n",
    "            \n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "\n",
    "    # Print the input-text.\n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "    # Print the translated output-text.\n",
    "    print(\"Translated text:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    # Optionally print the true translated text.\n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Som De kan se, indfandt det store \"år 2000-problem\" sig ikke. Til gengæld har borgerne i en del af medlemslandene været ramt af meget forfærdelige naturkatastrofer.\n",
      "\n",
      "Translated text:\n",
      " tilladelsesforbehold tilladelsesforbehold retsorden overvejelse tjenestemændenes tjenestemændenes begået begået begået medtages medtages begået begået begået etiske medtages begået begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske etiske begået etiske\n",
      "\n",
      "True output text:\n",
      "ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "translate(input_text=data_src[idx],\n",
    "          true_output_text=data_dest[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
